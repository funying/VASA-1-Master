# VASA-1 Implementation

This repository contains an unofficial implementation of the VASA-1 model, focused on generating lifelike talking faces with visual affective skills using a single static image and a speech audio clip. The implementation specifically addresses expressive and disentangled face latent space encoding and holistic facial dynamics generation using diffusion transformers.

## Disclaimer
This is an *unofficial* implementation and has not been endorsed by the original authors of the VASA-1 model. It is designed to facilitate understanding and application of the concepts presented in the original research and is provided "as is", without warranty of any kind.

## Repository Structure

- `/MegaPortraits`: Contains all the implementations related to the expressive and disentangled face latent space encoding.
- `/DiffusionTransformer`: Contains the implementation of the diffusion transformer used for generating facial dynamics.

Other files in the repository handle general functionalities like inference, data preprocessing, and documentation.

## Installation

To set up this project, clone the repository and install the required dependencies:

```bash
git clone [https://github.com/yourusername/vasa-1-implementation.git](https://github.com/vishwaraghava009/VASA-1-Master.git)
cd VASA-1-Master
# cd to respective directory(if you want to access the particular work)

*Note* : You might need to set the google drive according to the code to store intermdiate chunks.
